{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "40968410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# To set your enviornment variables in your terminal run the following line:\n",
    "# export 'BEARER_TOKEN'='<your_bearer_token>'\n",
    "os.environ[\"BEARER_TOKEN\"] = \"AAAAAAAAAAAAAAAAAAAAACfadAEAAAAArANTZpoNHhO8mC%2ByIs3OngWhwyk%3DkOMsCWKWvyFmcwZb1mEstEMIdb03OobXzNsgHpU87SXWvWct0C\"\n",
    "\n",
    "bearer_token = os.environ.get(\"BEARER_TOKEN\")\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2FilteredStreamPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_rules():\n",
    "    response = requests.get(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream/rules\", auth=bearer_oauth\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Cannot get rules (HTTP {}): {}\".format(response.status_code, response.text)\n",
    "        )\n",
    "    print(json.dumps(response.json()))\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def delete_all_rules(rules):\n",
    "    if rules is None or \"data\" not in rules:\n",
    "        return None\n",
    "\n",
    "    ids = list(map(lambda rule: rule[\"id\"], rules[\"data\"]))\n",
    "    payload = {\"delete\": {\"ids\": ids}}\n",
    "    response = requests.post(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream/rules\",\n",
    "        auth=bearer_oauth,\n",
    "        json=payload\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Cannot delete rules (HTTP {}): {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    print(json.dumps(response.json()))\n",
    "\n",
    "\n",
    "def set_rules(delete):\n",
    "    # You can adjust the rules if needed\n",
    "    sample_rules = [\n",
    "        {\"value\": \"dog has:images\", \"tag\": \"dog pictures\"},\n",
    "        {\"value\": \"cat has:images -grumpy\", \"tag\": \"cat pictures\"},\n",
    "    ]\n",
    "    payload = {\"add\": sample_rules}\n",
    "    response = requests.post(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream/rules\",\n",
    "        auth=bearer_oauth,\n",
    "        json=payload,\n",
    "    )\n",
    "    if response.status_code != 201:\n",
    "        raise Exception(\n",
    "            \"Cannot add rules (HTTP {}): {}\".format(response.status_code, response.text)\n",
    "        )\n",
    "    print(json.dumps(response.json()))\n",
    "\n",
    "\n",
    "def get_stream(countOfTweets):\n",
    "    countOfTweets = int(countOfTweets)\n",
    "    response = requests.get(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream\", auth=bearer_oauth, stream=True,\n",
    "    )\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Cannot get stream (HTTP {}): {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    all_responses = []\n",
    "    count = 0\n",
    "    for response_line in response.iter_lines():\n",
    "        if count >= countOfTweets:\n",
    "            break;\n",
    "\n",
    "        if response_line:\n",
    "            # get the current tweet json\n",
    "            json_response = json.loads(response_line)    \n",
    " \n",
    "            # Keep count of the processed tweets\n",
    "            count = count + 1 \n",
    "\n",
    "            # initialize a dict to hold the current tweet\n",
    "            tweet = {}\n",
    "\n",
    "            # extract the data from the tweet and store it in our variable\n",
    "            tweet['count'] = count\n",
    "            tweet['id'] = json_response[\"data\"][\"id\"]\n",
    "            tweet[\"text\"] = json_response[\"data\"][\"text\"]\n",
    "            tweet[\"tag\"] = json_response[\"matching_rules\"][0][\"tag\"]\n",
    "\n",
    "\n",
    "            # add the tweet to our response list\n",
    "            all_responses.append(tweet)\n",
    "\n",
    "            #print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "            print('tweets streamed: ' + str(count))\n",
    "    #take the streamed responses, put it into a dataframe and print the dataframe        \n",
    "    #df = pd.DataFrame(all_responses)\n",
    "    #print(df)\n",
    "\n",
    "    #return the streamed responses         \n",
    "    return(all_responses)\n",
    "\n",
    "\n",
    "def main():\n",
    "    rules = get_rules()\n",
    "    delete = delete_all_rules(rules)\n",
    "    set = set_rules(delete)\n",
    "    results = get_stream(100)\n",
    "    return(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "39922267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "\n",
    "# Create the spark session\n",
    "spark = SparkSession.builder.appName(\"Twitter_Sentiment_NLP\").getOrCreate()\n",
    "\n",
    "#Pre-Load the classifier and the model\n",
    "# Load the saved NaiveBayes Classifier\n",
    "nb = NaiveBayes.load(\"../static/resources/nb\")\n",
    "\n",
    "#Restored the trained predictor (Trained on 1 mil tweets)\n",
    "predictor = NaiveBayesModel.load(\"../static/resources/nb_model\")\n",
    "\n",
    "def eval_text_single(text, polarity = 1.0):\n",
    "    list = [\n",
    "    {\"polarity\": polarity, \"text\" : text}\n",
    "    ]\n",
    "\n",
    "    # The pipeline doesn't work as well when it it just one record in the list, so creating a fake list and adding the request to it.\n",
    "    text_list = [{\"text\": \"I am so happy for this text!  I can now have everything I want.\", \"polarity\": 1.0},\n",
    "             {\"text\": \"This sucks!  I don't like this anymore.\", \"polarity\": 0.0},\n",
    "             {\"text\" : \"This is a bad text.\", \"polarity\" : 0.0},\n",
    "             {\"text\": \"I love you.\", \"polarity\": 0.0},\n",
    "             {\"text\": \"Wow!  I can't believe how great this is.\", \"polarity\": 0.0},\n",
    "            ]\n",
    "    \n",
    "    # Add to the fake list\n",
    "    text_list.append({\"text\" : text, \"polarity\" : polarity})\n",
    "    \n",
    "    tweet_df = spark.createDataFrame(text_list)\n",
    "\n",
    "    # Create a length column to be used as a future feature\n",
    "    data_df = tweet_df.withColumn('length', length(tweet_df['text']))\n",
    "\n",
    "    # Create all the features to the data set\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "    stopRemove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "    hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "    idf = IDF(inputCol='hash_token', outputCol='idf_token')\n",
    "\n",
    "    # Create feature vectors\n",
    "    clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')\n",
    "\n",
    "    # Create and run a data processing Pipeline\n",
    "    data_prep_pipeline = Pipeline(stages=[tokenizer, stopRemove, hashingTF, idf, clean_up])\n",
    "\n",
    "    # Fit and transform the pipeline\n",
    "    cleaner = data_prep_pipeline.fit(data_df)\n",
    "    cleaned = cleaner.transform(data_df)\n",
    "\n",
    "    # Load the saved NaiveBayes Classifier\n",
    "    #nb = NaiveBayes.load(\"static/resources/nb\")\n",
    "\n",
    "    #Restored the trained predictor (Trained on 1 mil tweets)\n",
    "    #predictor = NaiveBayesModel.load(\"static/resources/nb_model\")\n",
    "\n",
    "    #Predict the sentiment of the text using the restored predictor\n",
    "    test_results = predictor.transform(cleaned)\n",
    "\n",
    "    df = test_results.select(\"text\",\"prediction\", \"probability\").toPandas()\n",
    "\n",
    "    positives = [prob[1] for prob in df['probability']]\n",
    "    df['probability'] = positives\n",
    "    \n",
    "    #Prepare the results, show the first row \n",
    "    result = {}\n",
    "    result[\"text\"] = df[\"text\"][5]\n",
    "    result[\"prediction\"] = df[\"prediction\"][5]\n",
    "    result[\"probability\"] = df[\"probability\"][5]\n",
    "\n",
    "    if result[\"prediction\"] == 1:\n",
    "        result[\"prediction\"] = \"Positive\"\n",
    "    else: \n",
    "        result[\"prediction\"] = \"Negative\"\n",
    "\n",
    "    return(result)\n",
    "\n",
    "def eval_text_list(text_list):\n",
    "\n",
    "    tweet_df = spark.createDataFrame(text_list)\n",
    "\n",
    "    # Create a length column to be used as a future feature\n",
    "    data_df = tweet_df.withColumn('length', length(tweet_df['text']))\n",
    "\n",
    "    # Create all the features to the data set\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "    stopRemove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "    hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "    idf = IDF(inputCol='hash_token', outputCol='idf_token')\n",
    "\n",
    "    # Create feature vectors\n",
    "    clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')\n",
    "\n",
    "    # Create and run a data processing Pipeline\n",
    "    data_prep_pipeline = Pipeline(stages=[tokenizer, stopRemove, hashingTF, idf, clean_up])\n",
    "\n",
    "    # Fit and transform the pipeline\n",
    "    cleaner = data_prep_pipeline.fit(data_df)\n",
    "    cleaned = cleaner.transform(data_df)\n",
    "\n",
    "    # Load the saved NaiveBayes Classifier\n",
    "    #nb = NaiveBayes.load(\"static/resources/nb\")\n",
    "\n",
    "    #Restored the trained predictor (Trained on 1 mil tweets)\n",
    "    #predictor = NaiveBayesModel.load(\"static/resources/nb_model\")\n",
    "\n",
    "    #Predict the sentiment of the text using the restored predictor\n",
    "    test_results = predictor.transform(cleaned)\n",
    "\n",
    "    df = test_results.select(\"text\", \"tag\", \"prediction\", \"probability\").toPandas()\n",
    "\n",
    "    positive_score = [prob[1] for prob in df['probability']]\n",
    "    \n",
    "    df['probability'] = positive_score\n",
    "\n",
    "    #Format the percentage \n",
    "    percents = [\"{:.2%}\".format(prob) for prob in df['probability']]\n",
    "    df['percent'] = percents\n",
    "\n",
    "    #Rename the prediction to a text value\n",
    "    df.loc[df['prediction'] == 1.0, 'prediction'] = 'Positive'\n",
    "    df.loc[df['prediction'] == 0.0, 'prediction'] = 'Negative'\n",
    "\n",
    "    # pull out the top and bottom 10 records.  \n",
    "    top_10 = df.sort_values(by=['probability'], ascending=False).head(10)\n",
    "    bottom_10 = df.sort_values(by=['probability'], ascending=True).head(10)\n",
    "    \n",
    "    return(df, top_10.to_dict('records'), bottom_10.to_dict('records'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7e70903f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\": [{\"id\": \"1534331066772619264\", \"value\": \"cat has:images -grumpy\", \"tag\": \"cat pictures\"}, {\"id\": \"1534331066772619265\", \"value\": \"dog has:images\", \"tag\": \"dog pictures\"}], \"meta\": {\"sent\": \"2022-06-08T01:25:24.280Z\", \"result_count\": 2}}\n",
      "{\"meta\": {\"sent\": \"2022-06-08T01:25:24.489Z\", \"summary\": {\"deleted\": 2, \"not_deleted\": 0}}}\n",
      "{\"data\": [{\"value\": \"cat has:images -grumpy\", \"tag\": \"cat pictures\", \"id\": \"1534345818785730565\"}, {\"value\": \"dog has:images\", \"tag\": \"dog pictures\", \"id\": \"1534345818785730564\"}], \"meta\": {\"sent\": \"2022-06-08T01:25:24.673Z\", \"summary\": {\"created\": 2, \"not_created\": 0, \"valid\": 2, \"invalid\": 0}}}\n",
      "200\n",
      "tweets streamed: 1\n",
      "tweets streamed: 2\n",
      "tweets streamed: 3\n",
      "tweets streamed: 4\n",
      "tweets streamed: 5\n",
      "tweets streamed: 6\n",
      "tweets streamed: 7\n",
      "tweets streamed: 8\n",
      "tweets streamed: 9\n",
      "tweets streamed: 10\n",
      "tweets streamed: 11\n",
      "tweets streamed: 12\n",
      "tweets streamed: 13\n",
      "tweets streamed: 14\n",
      "tweets streamed: 15\n",
      "tweets streamed: 16\n",
      "tweets streamed: 17\n",
      "tweets streamed: 18\n",
      "tweets streamed: 19\n",
      "tweets streamed: 20\n",
      "tweets streamed: 21\n",
      "tweets streamed: 22\n",
      "tweets streamed: 23\n",
      "tweets streamed: 24\n",
      "tweets streamed: 25\n",
      "tweets streamed: 26\n",
      "tweets streamed: 27\n",
      "tweets streamed: 28\n",
      "tweets streamed: 29\n",
      "tweets streamed: 30\n",
      "tweets streamed: 31\n",
      "tweets streamed: 32\n",
      "tweets streamed: 33\n",
      "tweets streamed: 34\n",
      "tweets streamed: 35\n",
      "tweets streamed: 36\n",
      "tweets streamed: 37\n",
      "tweets streamed: 38\n",
      "tweets streamed: 39\n",
      "tweets streamed: 40\n",
      "tweets streamed: 41\n",
      "tweets streamed: 42\n",
      "tweets streamed: 43\n",
      "tweets streamed: 44\n",
      "tweets streamed: 45\n",
      "tweets streamed: 46\n",
      "tweets streamed: 47\n",
      "tweets streamed: 48\n",
      "tweets streamed: 49\n",
      "tweets streamed: 50\n",
      "tweets streamed: 51\n",
      "tweets streamed: 52\n",
      "tweets streamed: 53\n",
      "tweets streamed: 54\n",
      "tweets streamed: 55\n",
      "tweets streamed: 56\n",
      "tweets streamed: 57\n",
      "tweets streamed: 58\n",
      "tweets streamed: 59\n",
      "tweets streamed: 60\n",
      "tweets streamed: 61\n",
      "tweets streamed: 62\n",
      "tweets streamed: 63\n",
      "tweets streamed: 64\n",
      "tweets streamed: 65\n",
      "tweets streamed: 66\n",
      "tweets streamed: 67\n",
      "tweets streamed: 68\n",
      "tweets streamed: 69\n",
      "tweets streamed: 70\n",
      "tweets streamed: 71\n",
      "tweets streamed: 72\n",
      "tweets streamed: 73\n",
      "tweets streamed: 74\n",
      "tweets streamed: 75\n",
      "tweets streamed: 76\n",
      "tweets streamed: 77\n",
      "tweets streamed: 78\n",
      "tweets streamed: 79\n",
      "tweets streamed: 80\n",
      "tweets streamed: 81\n",
      "tweets streamed: 82\n",
      "tweets streamed: 83\n",
      "tweets streamed: 84\n",
      "tweets streamed: 85\n",
      "tweets streamed: 86\n",
      "tweets streamed: 87\n",
      "tweets streamed: 88\n",
      "tweets streamed: 89\n",
      "tweets streamed: 90\n",
      "tweets streamed: 91\n",
      "tweets streamed: 92\n",
      "tweets streamed: 93\n",
      "tweets streamed: 94\n",
      "tweets streamed: 95\n",
      "tweets streamed: 96\n",
      "tweets streamed: 97\n",
      "tweets streamed: 98\n",
      "tweets streamed: 99\n",
      "tweets streamed: 100\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c10c568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#include json library\n",
    "import json\n",
    "\n",
    "#json string data\n",
    "employee_string = '{\"rules\" : [{\"value\": \"dog has:images\", \"tag\": \"dog pictures\"},{\"value\": \"cat has:images -grumpy\", \"tag\": \"cat pictures\"}]}'\n",
    "\n",
    "#convert string to  object\n",
    "json_object = json.loads(employee_string)\n",
    "\n",
    "#check new data type\n",
    "print(type(json_object[\"rules\"]))\n",
    "\n",
    "#output\n",
    "#<class 'dict'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fd8f560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/07 19:39:40 WARN DAGScheduler: Broadcasting large task binary with size 8.1 MiB\n"
     ]
    }
   ],
   "source": [
    "df, top_10, bottom_10 = eval_text_list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c9a024ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plot_data': {'tags': ['cat pictures', 'dog pictures'], 'positives': [41, 11], 'negatives': [31, 17]}, 'tags': ['cat pictures', 'dog pictures'], 'totals': {'predictions': ['Negative', 'Positive'], 'counts': [48, 52]}, 'total_count': 100, 'positive_count': 52}\n"
     ]
    }
   ],
   "source": [
    "def format_results_for_plotting(df):\n",
    "\n",
    "    #df with the total count\n",
    "    totals = df.groupby(['prediction']).size().reset_index(name='counts')\n",
    "\n",
    "    #list with the distinct tags\n",
    "    tags = df.groupby(['tag']).size().reset_index(name='counts')['tag'].to_list()\n",
    "\n",
    "    #list with the distinct predictions\n",
    "    predictions = df.groupby(['prediction']).size().reset_index(name='counts')['prediction'].to_list()\n",
    "\n",
    "    # create a df with the aggregate counts\n",
    "    agg_bytag = df.groupby(['tag','prediction']).size().reset_index(name='counts')\n",
    "\n",
    "\n",
    "    #Create lists for the positive and negative counts\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    for tag in tags:\n",
    "        for prediction in predictions:\n",
    "            if prediction == \"Positive\":\n",
    "                try:\n",
    "                    pos = int(agg_bytag.loc[(agg_bytag[\"tag\"] == tag) & (agg_bytag[\"prediction\"] == prediction)]['counts'].values[0])\n",
    "                except:\n",
    "                    pos = int(0)\n",
    "                positives.append(pos)\n",
    "            if prediction == \"Negative\":\n",
    "                try:\n",
    "                    neg = int(agg_bytag.loc[(agg_bytag[\"tag\"] == tag) & (agg_bytag[\"prediction\"] == prediction)]['counts'].values[0]) \n",
    "                except:\n",
    "                    neg = int(0)\n",
    "                negatives.append(neg)\n",
    "\n",
    "    #Create the dataset for the bar graph\n",
    "    plot_data = {} \n",
    "    plot_data['tags'] = tags\n",
    "    plot_data['positives'] = positives\n",
    "    plot_data['negatives'] = negatives   \n",
    "\n",
    "    #Create the full dataset for plotting\n",
    "    data = {}\n",
    "    data[\"plot_data\"] = plot_data\n",
    "    data[\"tags\"] = tags\n",
    "    data[\"totals\"]={}\n",
    "    data[\"totals\"][\"predictions\"] = totals['prediction'].to_list()\n",
    "    data[\"totals\"][\"counts\"] = totals['counts'].to_list()\n",
    "    data[\"total_count\"] = int(totals['counts'].sum())\n",
    "    data[\"positive_count\"] = int(sum(positives))\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "data = format_results_for_plotting(df)\n",
    "print(data)\n",
    "with open('../static/resources/evaluated_tweets.json', 'w') as fp:\n",
    "    json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b8a5f362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prediction': 'Negative', 'counts': 48},\n",
       " {'prediction': 'Positive', 'counts': 52}]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b0bb4281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"tags\": [\n",
      "        \"cat pictures\",\n",
      "        \"dog pictures\"\n",
      "    ],\n",
      "    \"positives\": [\n",
      "        41,\n",
      "        11\n",
      "    ],\n",
      "    \"negatives\": [\n",
      "        31,\n",
      "        17\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Serializing json  \n",
    "json_object = json.dumps(plot_data, indent = 4) \n",
    "print(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635bcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv2",
   "language": "python",
   "name": "mlenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
