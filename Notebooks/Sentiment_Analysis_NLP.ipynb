{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44388ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2766026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Twitter_Sentiment_NLP\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://ilanp-bucket.s3.us-west-2.amazonaws.com/sentiment_analysis_10k.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "tweet_df = spark.read.csv(SparkFiles.get(\"sentiment_analysis_10k.csv\"), sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d057ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [\n",
    "    {\"polarity\": 1.0, \"label\" : 1.0, \"text\" : \"I am so happy to be here today!\"},\n",
    "    {\"polarity\" : 0.0,\"label\" : 0.0, \"text\" : \"Today is a terrible day.\"},\n",
    "    {\"polarity\" : 1.0,\"label\" : 1.0, \"text\" : \"I am so in love today!\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df2 = spark.createDataFrame(list)\n",
    "#tweet_df2 = tweet_df\n",
    "tweet_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a13a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26edb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "# Create a length column to be used as a future feature\n",
    "data_df = tweet_df2.withColumn('length', length(tweet_df2['text']))\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80333ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all the features to the data set\n",
    "#pos_neg_to_num = StringIndexer(inputCol='polarity',outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "# Create feature vectors\n",
    "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f2aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(stages=[tokenizer, stopremove, hashingTF, idf, clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e83905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the pipeline\n",
    "cleaner = data_prep_pipeline.fit(data_df)\n",
    "cleaned = cleaner.transform(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebff9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634234f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned.select(['polarity','label','features']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break data down into a training set and a testing set\n",
    "#training, testing = cleaned.randomSplit([0.7, 0.3], 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed7b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "# Create a Naive Bayes model and fit training data\n",
    "#nb = NaiveBayes()\n",
    "#predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea88b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb_path = \"./nb\"\n",
    "#nb.save(nb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb8df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "# Restore the saved NaiveBayes classifier\n",
    "nb2 = NaiveBayes.load(\"./nb\")\n",
    "nb2.getSmoothing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a404bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor.save(\"./nb_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restored the trained predictor\n",
    "predictor2 = NaiveBayesModel.load(\"./nb_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8efd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = predictor2.transform(cleaned)\n",
    "test_results.show(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "acc_eval = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='prediction')\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting Text Sentiment was: %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0882fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_results.select(\"text\",\"label\",\"prediction\", \"probability\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9897645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb210aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(df[\"label\"], df[\"prediction\"])\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"],\n",
    "    columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "\n",
    "# Displaying results\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2943aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store environmental variable\n",
    "from getpass import getpass\n",
    "password = getpass('Provide Password')\n",
    "\n",
    "# Configure settings for RDS\n",
    "mode = \"overwrite\"\n",
    "jdbc_url=\"jdbc:postgresql://database-1.c3f2jo4rdylg.us-west-2.rds.amazonaws.com:5432/sentiment_analysis\"\n",
    "config = {\"user\":\"postgres\", \n",
    "          \"password\": password, \n",
    "          \"driver\":\"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results[\"polarity\",'text','new_date',\"length\",\"label\", \"token_text\",\"features\", \"prediction\"].show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in test_results.dtypes:\n",
    "    print(col[0]+\" , \"+col[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws\n",
    "test_results2 = test_results.withColumn(\"token_text\", concat_ws(\",\",col(\"token_text\")))\n",
    "test_results3 = test_results2.withColumn(\"stop_tokens\", concat_ws(\",\",col(\"stop_tokens\")))\n",
    "test_results3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to active_user table in RDS\n",
    "test_results['polarity','text','new_date',\"length\",\"label\", \"token_text\", \"prediction\"].write.jdbc(url=jdbc_url, table='test_results', mode=mode, properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3ba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from flask import session\n",
    "\n",
    "\n",
    "# To set your enviornment variables in your terminal run the following line:\n",
    "# export 'BEARER_TOKEN'='<your_bearer_token>'\n",
    "with open('../myconfig.json','r') as fh:\n",
    "    config = json.load(fh)\n",
    "os.environ[\"BEARER_TOKEN\"] = config[\"BEARER_TOKEN\"]\n",
    "\n",
    "bearer_token = os.environ.get(\"BEARER_TOKEN\")\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2FilteredStreamPython\"\n",
    "    return r\n",
    "\n",
    "def create_rules(hashtag_data):\n",
    "        new_rules = '{\"rules\" : ['\n",
    "        counter = 0\n",
    "        for hashtag in hashtag_data['tw_trends']:\n",
    "            counter = counter + 1\n",
    "            if counter == 1:\n",
    "                new_rules = new_rules + '{\"value\": \"' + hashtag['hashtag'] + ' -is:retweet lang:en -has:links -has:media\", \"tag\": \"' + hashtag['hashtag'] + '\"}'\n",
    "            else:\n",
    "                new_rules = new_rules + ',{\"value\": \"' + hashtag['hashtag'] + ' -is:retweet lang:en -has:links -has:media\", \"tag\": \"' + hashtag['hashtag'] + '\"}'\n",
    "        new_rules = new_rules + ']}'        \n",
    "        print(new_rules)\n",
    "        return(new_rules)\n",
    "\n",
    "def get_rules():\n",
    "    response = requests.get(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream/rules\", auth=bearer_oauth\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Cannot get rules (HTTP {}): {}\".format(response.status_code, response.text)\n",
    "        )\n",
    "    print('get_rules() response:')    \n",
    "    print(json.dumps(response.json()))\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def delete_all_rules(rules):\n",
    "    if rules is None or \"data\" not in rules:\n",
    "        return None\n",
    "\n",
    "    ids = list(map(lambda rule: rule[\"id\"], rules[\"data\"]))\n",
    "    payload = {\"delete\": {\"ids\": ids}}\n",
    "    response = requests.post(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream/rules\",\n",
    "        auth=bearer_oauth,\n",
    "        json=payload\n",
    "    )\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Cannot delete rules (HTTP {}): {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    print('delete_all_rules(rules) response:')    \n",
    "    \n",
    "    (json.dumps(response.json()))\n",
    "\n",
    "\n",
    "def set_rules(rules):\n",
    "    # You can adjust the rules if needed \n",
    "    # if the passed in rules is null, then \n",
    "    if rules is None:\n",
    "        sample_rules = [\n",
    "            {\"value\": \"dog has:images\", \"tag\": \"dog pictures\"},\n",
    "            {\"value\": \"cat has:images -grumpy\", \"tag\": \"cat pictures\"},\n",
    "        ]\n",
    "    else:\n",
    "        print('assigning specified rules')\n",
    "        sample_rules = rules\n",
    "    payload = {\"add\": sample_rules}\n",
    "    response = requests.post(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream/rules\",\n",
    "        auth=bearer_oauth,\n",
    "        json=payload,\n",
    "    )\n",
    "    if response.status_code != 201:\n",
    "        raise Exception(\n",
    "            \"Cannot add rules (HTTP {}): {}\".format(response.status_code, response.text)\n",
    "        )\n",
    "    print('set_rules(delete) response:')      \n",
    "    print(json.dumps(response.json()))\n",
    "\n",
    "\n",
    "def get_stream(countOfTweets):\n",
    "    countOfTweets = int(countOfTweets)\n",
    "    response = requests.get(\n",
    "        \"https://api.twitter.com/2/tweets/search/stream\", auth=bearer_oauth, stream=True,\n",
    "    )\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Cannot get stream (HTTP {}): {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    all_responses = []\n",
    "    count = 0\n",
    "    for response_line in response.iter_lines():\n",
    "        if count >= countOfTweets:\n",
    "            break;\n",
    "\n",
    "        if response_line:\n",
    "            # get the current tweet json\n",
    "            json_response = json.loads(response_line)    \n",
    " \n",
    "            # Keep count of the processed tweets\n",
    "            count = count + 1 \n",
    "\n",
    "            # initialize a dict to hold the current tweet\n",
    "            tweet = {}\n",
    "\n",
    "            # extract the data from the tweet and store it in our variable\n",
    "            tweet['count'] = count\n",
    "            tweet['id'] = json_response[\"data\"][\"id\"]\n",
    "            tweet[\"text\"] = json_response[\"data\"][\"text\"]\n",
    "            tweet[\"tag\"] = json_response[\"matching_rules\"][0][\"tag\"]\n",
    "\n",
    "            # Update Session with current tweet.\n",
    "            #session['current_tweet'] = tweet \n",
    "\n",
    "            # add the tweet to our response list\n",
    "            all_responses.append(tweet)\n",
    "\n",
    "            #print(json.dumps(json_response, indent=4, sort_keys=True))\n",
    "            print('tweets streamed: ' + str(count))\n",
    "    #take the streamed responses, put it into a dataframe and print the dataframe        \n",
    "    #df = pd.DataFrame(all_responses)\n",
    "    #print(df)\n",
    "\n",
    "    #return the streamed responses         \n",
    "    return(all_responses)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "def main():\n",
    "    rules = get_rules()\n",
    "    delete = delete_all_rules(rules)\n",
    "    set = set_rules(delete)\n",
    "    get_stream(set) \n",
    "    \n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3445b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes, NaiveBayesModel\n",
    "\n",
    "# Create the spark session\n",
    "spark = SparkSession.builder.appName(\"Twitter_Sentiment_NLP\").getOrCreate()\n",
    "\n",
    "#Pre-Load the classifier and the model\n",
    "# Load the saved NaiveBayes Classifier\n",
    "nb = NaiveBayes.load(\"../static/resources/nb\")\n",
    "\n",
    "#Restored the trained predictor (Trained on 1 mil tweets)\n",
    "predictor = NaiveBayesModel.load(\"../static/resources/nb_model\")\n",
    "\n",
    "def eval_text_single(text, polarity = 1.0):\n",
    "    list = [\n",
    "    {\"polarity\": polarity, \"text\" : text}\n",
    "    ]\n",
    "\n",
    "    # The pipeline doesn't work as well when it it just one record in the list, so creating a fake list and adding the request to it.\n",
    "    text_list = [{\"text\": \"I am so happy for this text!  I can now have everything I want.\", \"polarity\": 1.0},\n",
    "             {\"text\": \"This sucks!  I don't like this anymore.\", \"polarity\": 0.0},\n",
    "             {\"text\" : \"This is a bad text.\", \"polarity\" : 0.0},\n",
    "             {\"text\": \"I love you.\", \"polarity\": 0.0},\n",
    "             {\"text\": \"Wow!  I can't believe how great this is.\", \"polarity\": 0.0},\n",
    "            ]\n",
    "    \n",
    "    # Add to the fake list\n",
    "    text_list.append({\"text\" : text, \"polarity\" : polarity})\n",
    "    \n",
    "    tweet_df = spark.createDataFrame(text_list)\n",
    "\n",
    "    # Create a length column to be used as a future feature\n",
    "    data_df = tweet_df.withColumn('length', length(tweet_df['text']))\n",
    "\n",
    "    # Create all the features to the data set\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "    stopRemove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "    hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "    idf = IDF(inputCol='hash_token', outputCol='idf_token')\n",
    "\n",
    "    # Create feature vectors\n",
    "    clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')\n",
    "\n",
    "    # Create and run a data processing Pipeline\n",
    "    data_prep_pipeline = Pipeline(stages=[tokenizer, stopRemove, hashingTF, idf, clean_up])\n",
    "\n",
    "    # Fit and transform the pipeline\n",
    "    cleaner = data_prep_pipeline.fit(data_df)\n",
    "    cleaned = cleaner.transform(data_df)\n",
    "\n",
    "    # Load the saved NaiveBayes Classifier\n",
    "    #nb = NaiveBayes.load(\"static/resources/nb\")\n",
    "\n",
    "    #Restored the trained predictor (Trained on 1 mil tweets)\n",
    "    #predictor = NaiveBayesModel.load(\"static/resources/nb_model\")\n",
    "\n",
    "    #Predict the sentiment of the text using the restored predictor\n",
    "    test_results = predictor.transform(cleaned)\n",
    "\n",
    "    df = test_results.select(\"text\",\"prediction\", \"probability\").toPandas()\n",
    "\n",
    "    positives = [prob[1] for prob in df['probability']]\n",
    "    df['probability'] = positives\n",
    "    \n",
    "    #Prepare the results, show the first row \n",
    "    result = {}\n",
    "    result[\"text\"] = df[\"text\"][5]\n",
    "    result[\"prediction\"] = df[\"prediction\"][5]\n",
    "    result[\"probability\"] = df[\"probability\"][5]\n",
    "\n",
    "    if result[\"prediction\"] == 1:\n",
    "        result[\"prediction\"] = \"Positive\"\n",
    "    else: \n",
    "        result[\"prediction\"] = \"Negative\"\n",
    "\n",
    "    return(result)\n",
    "\n",
    "def eval_text_list(text_list):\n",
    "\n",
    "    tweet_df = spark.createDataFrame(text_list)\n",
    "\n",
    "    # Create a length column to be used as a future feature\n",
    "    data_df = tweet_df.withColumn('length', length(tweet_df['text']))\n",
    "\n",
    "    # Create all the features to the data set\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "    stopRemove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "    hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "    idf = IDF(inputCol='hash_token', outputCol='idf_token')\n",
    "\n",
    "    # Create feature vectors\n",
    "    clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')\n",
    "\n",
    "    # Create and run a data processing Pipeline\n",
    "    data_prep_pipeline = Pipeline(stages=[tokenizer, stopRemove, hashingTF, idf, clean_up])\n",
    "\n",
    "    # Fit and transform the pipeline\n",
    "    cleaner = data_prep_pipeline.fit(data_df)\n",
    "    cleaned = cleaner.transform(data_df)\n",
    "\n",
    "    # Load the saved NaiveBayes Classifier\n",
    "    #nb = NaiveBayes.load(\"static/resources/nb\")\n",
    "\n",
    "    #Restored the trained predictor (Trained on 1 mil tweets)\n",
    "    #predictor = NaiveBayesModel.load(\"static/resources/nb_model\")\n",
    "\n",
    "    #Predict the sentiment of the text using the restored predictor\n",
    "    test_results = predictor.transform(cleaned)\n",
    "\n",
    "    df = test_results.select(\"text\", \"tag\", \"prediction\", \"probability\").toPandas()\n",
    "    \n",
    "    positive_score = [prob[1] for prob in df['probability']]\n",
    "    df['probability'] = positive_score\n",
    "\n",
    "    percents = [\"{:.2%}\".format(prob) for prob in df['probability']]\n",
    "    df['percent'] = percents\n",
    "\n",
    "    df.loc[df['prediction'] == 1.0, 'prediction'] = 'Positive'\n",
    "    df.loc[df['prediction'] == 0.0, 'prediction'] = 'Negative'\n",
    "\n",
    "    top_10 = df.sort_values(by=['probability'], ascending=False).head(10)\n",
    "    bottom_10 = df.sort_values(by=['probability'], ascending=True).head(10)\n",
    "    \n",
    "    return(df, top_10, bottom_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bffea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import tostring\n",
    "from flask import Flask, render_template, redirect, url_for, request, session\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "#pull the rules from the textarea input box\n",
    "rules = '{\"rules\" : [{\"value\": \"Apple -is:retweet lang:en -has:links -has:media\", \"tag\": \"Apple\"},{\"value\": \"#WWDC22 -is:retweet lang:en -has:links -has:media\", \"tag\": \"#WWDC22\"},{\"value\": \"#LoveIsland -is:retweet lang:en -has:links -has:media\", \"tag\": \"#LoveIsland\"},{\"value\": \"Proud Boys -is:retweet lang:en -has:links -has:media\", \"tag\": \"Proud Boys\"},{\"value\": \"iOS 16 -is:retweet lang:en -has:links -has:media\", \"tag\": \"iOS 16\"},{\"value\": \"Aaron Donald -is:retweet lang:en -has:links -has:media\", \"tag\": \"Aaron Donald\"},{\"value\": \"Wilbur -is:retweet lang:en -has:links -has:media\", \"tag\": \"Wilbur\"},{\"value\": \"Giant Bomb -is:retweet lang:en -has:links -has:media\", \"tag\": \"Giant Bomb\"},{\"value\": \"Jocelyn Alo -is:retweet lang:en -has:links -has:media\", \"tag\": \"Jocelyn Alo\"},{\"value\": \"Michigan -is:retweet lang:en -has:links -has:media\", \"tag\": \"Michigan\"}]}'\n",
    "\n",
    "#pull the number of Tweets to request from the Twitter API\n",
    "countOfTweets = 25\n",
    "print(f'Count of Tweets: {str(countOfTweets)}')\n",
    "if countOfTweets is None:\n",
    "    countOfTweets = 10\n",
    "\n",
    "print('rules: type: ' + rules)\n",
    "\n",
    "#Perform the steps needed to receive the twitter stream\n",
    "\n",
    "rules = json.loads(rules)\n",
    "#get the previous rules\n",
    "old_rules = get_rules()\n",
    "\n",
    "#delete the previous rules\n",
    "delete = delete_all_rules(old_rules)\n",
    "\n",
    "#set the rules to be the new rules\n",
    "set = set_rules(rules[\"rules\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the twitter stream with the requested rule set\n",
    "tweet_list = get_stream(countOfTweets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab155486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send the collected twitter feed to the machine learning model\n",
    "eval_list, top_10, bottom_10 = eval_text_list(tweet_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the returned eval_list\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d73b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f6afd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
